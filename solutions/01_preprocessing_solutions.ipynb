{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794bac7d-1acb-43a7-8e17-1c9fa6e007e1",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Part 1 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c60227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f446a0-61d7-4055-9acb-34b010f8572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Use pandas to import Tweets\n",
    "csv_path = '../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(csv_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8599eb-1e4d-43db-b62c-dce158a18ce0",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 1: Preprocessing with Multiple Steps\n",
    "\n",
    "So far we've learned a few preprocessing operations, let's put them together in a function! This function would be a handy one to refer to if you happen to work with some messy English text data, and you want to preprocess it with a single function. \n",
    "\n",
    "The example text data for challenge 1 has been read in. Write a function to:\n",
    "- Lowercase the text\n",
    "- Remove punctuation marks\n",
    "- Remove extra whitespace characters\n",
    "\n",
    "Feel free to recycle the codes we've used above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a63cf0",
   "metadata": {},
   "source": [
    "**Explicaci√≥n del C√≥digo**\n",
    "* Este c√≥digo abre un archivo de texto, lee su contenido completo y lo imprime en pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24bec1d6-ae54-46cb-9db8-0e56b92a30ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "challenge1_path = '../data/example1.txt'\n",
    "\n",
    "with open(challenge1_path, 'r') as file:\n",
    "    challenge1 = file.read()\n",
    "    \n",
    "print(challenge1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b610c",
   "metadata": {},
   "source": [
    "**Explicaci√≥n del C√≥digo**\n",
    "- Esta funci√≥n borra todos los signos de puntuaci√≥n del texto, dejando solo letras, n√∫meros y espacios.\n",
    "\n",
    "- **Explicaci√≥n paso a paso**\n",
    "    * from string import punctuation\n",
    "        - Importa una constante llamada punctuation, que contiene todos los caracteres de puntuaci√≥n comunes:\n",
    "        - !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    * Definici√≥n de la funci√≥n remove_punct(text)\n",
    "        - Entrada: una cadena de texto.\n",
    "        - Proceso:\n",
    "            - Recorre cada car√°cter del texto (for char in text).\n",
    "            - Si ese car√°cter no est√° en la lista de punctuation, lo guarda en la lista no_punct.\n",
    "        - Salida: devuelve un nuevo texto (text_no_punct) formado al unir (''.join) todos los caracteres sin puntuaci√≥n.\n",
    "    * Resultado: obtienes el texto original pero sin ning√∫n signo de puntuaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdafd4fc-a816-405f-bfc5-d08285fd4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "    '''Remove punctuation marks in input text'''\n",
    "    \n",
    "    # Select characters not in puncutaion\n",
    "    no_punct = []\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            no_punct.append(char)\n",
    "\n",
    "    # Join the characters into a string\n",
    "    text_no_punct = ''.join(no_punct)   \n",
    "    \n",
    "    return text_no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260273f4",
   "metadata": {},
   "source": [
    "**Explicaci√≥n del C√≥digo**\n",
    "\n",
    "Ese c√≥digo implementa una funci√≥n de limpieza de texto b√°sica en NLP üßπ. Convierte el texto a min√∫sculas, elimina la puntuaci√≥n y normaliza los espacios en blanco.\n",
    "- Explicaci√≥n:\n",
    "    \n",
    "    1 Expresi√≥n regular para espacios: blankspace_pattern = r'\\s+'\n",
    "    - \\s = cualquier espacio en blanco (espacios, tabs, saltos de l√≠nea).\n",
    "    - (+) = uno o m√°s.\n",
    "    + üëâ Este patr√≥n detecta cualquier secuencia de espacios en blanco repetidos.\n",
    "            \n",
    "    2 Texto de reemplazo:\n",
    "    - blankspace_repl = ' '\n",
    "    - Significa que cada secuencia de espacios ser√° reemplazada por un solo espacio.\n",
    "\n",
    "    3 Funci√≥n clean_text:\n",
    "    - Paso 1: text.lower() - Convierte todo a min√∫sculas ‚Üí uniformidad.\n",
    "    - Paso 2: remove_punct(text) - Elimina signos de puntuaci√≥n (usa la funci√≥n que definiste antes).\n",
    "    - Paso 3: \n",
    "        - re.sub(...): reemplaza secuencias de espacios (m√∫ltiples) por uno solo.\n",
    "        - .strip(): quita espacios al inicio y al final.\n",
    "    \n",
    "    4 Devuelve el texto limpio ‚úÖ\n",
    "    5 Ejecuta la limpieza sobre challenge1, que es el texto que le√≠ste desde tu archivo example1.txt.\n",
    "\n",
    "üß™ Ejemplo pr√°ctico\n",
    "- Si challenge1 = \"Hola!!! Mundo, Esto es una PRUEBA...\"\n",
    "    - print(clean_text(challenge1))\n",
    "- Salida:\n",
    "    - hola mundo esto es una prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f8773e7-208c-4272-97e2-061c97860438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a text file that has some extra blankspace at the start and end blankspace is a catchall term for spaces tabs newlines and a bunch of other things that computers distinguish but to us all look like spaces tabs and newlines the python method called strip only catches blankspace at the start and end of a string but it wont catch it in the middle for example in this sentence once again regular expressions will help us with this'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a pattern in regex\n",
    "blankspace_pattern = r'\\s+'\n",
    "\n",
    "# Write a replacement for the pattern identfied\n",
    "blankspace_repl = ' '\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Step 1: Lowercase the input text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 2: Use remove_punct to remove puncutuation marks\n",
    "    text = remove_punct(text)\n",
    "\n",
    "    # Step 3: Remove extra whitespace characters\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "    \n",
    "clean_text(challenge1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24582e",
   "metadata": {},
   "source": [
    "üìå # **Funci√≥n clean_text**\n",
    "\n",
    "* Funci√≥n para:\n",
    "    * Convertir el texto a min√∫sculas\n",
    "    * Eliminar los signos de puntuaci√≥n\n",
    "    * Eliminar los espacios en blanco adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e996ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola mundo esto es una prueba\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# patrones para limpiar\n",
    "blankspace_pattern = r'\\s+'\n",
    "blankspace_repl = \" \"\n",
    "\n",
    "def remove_punct(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Step 1: lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: remove punctuation\n",
    "    text = remove_punct(text)\n",
    "    \n",
    "    # Step 3: remove extra whitespace\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Ejemplo de uso\n",
    "challenge1 = \"Hola!!!   Mundo,   esto es   una   PRUEBA...\"\n",
    "print(clean_text(challenge1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57a143-8876-4305-b3f1-c77001296919",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 2: Remove Stop Words\n",
    "\n",
    "We have known how `nltk` and `spaCy` work as NLP packages. We've also demostrated how to identify stop words with each package. \n",
    "\n",
    "Let's write **two** functions to remove stop words from our text data. \n",
    "\n",
    "- Complete the function for stop words removal using `nltk`\n",
    "    - The starter code requires two arguments: the raw text input and a list of predefined stop words\n",
    "- Complete the function for stop words removal using `spaCy`\n",
    "    - The starter code requires one argument: the raw text input\n",
    " \n",
    "A friendly reminder before we dive in: both functions take raw text as input‚Äîthat's a signal to perform tokenization on the raw text first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f743402-ace4-4b3b-8175-ba8b7ae87197",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopword_nltk(raw_text, stopword):\n",
    "    \n",
    "    # Step 1: Tokenization with nltk\n",
    "    tokens = word_tokenize(raw_text)\n",
    "    \n",
    "    # Step 2: Filter out tokens in the stop word list\n",
    "    text = [token for token in tokens if token not in stopword]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c052ec5-2f03-4c45-9f3e-ac0bbf29da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def remove_stopword_spacy(raw_text):\n",
    "\n",
    "    # Step 1: Apply the nlp pipeline\n",
    "    doc = nlp(raw_text)\n",
    "    \n",
    "    # Step 2: Filter out tokens in the stop word list\n",
    "    text = [token.text for token in doc if token.is_stop is False]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93c65-7fb2-40c5-9fc4-bb63fba5c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tweets['text'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1077f4d-e1d5-4d22-a05a-5b8370888aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopword_nltk(text, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4a692-fa53-4406-a7f7-3ee5e7e9811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopword_spacy(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9a6e3-4658-4556-9bc6-40e58b2045cd",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 3: Find the Word Boundary\n",
    "\n",
    "Now that we know BERT tokenization would often return subwords. Let's try a few more examples! \n",
    "\n",
    "Does the result make sense to you? What do you think is the correct word boundary to split the following words into subwords? \n",
    "\n",
    "Also feel free to read more about limitations of the WordPiece algorithm. For instance, [this blog post](https://medium.com/@rickbattle/weaknesses-of-wordpiece-tokenization-eb20e37fec99) dives into why would it fail, and [this one](https://tinkerd.net/blog/machine-learning/bert-tokenization/#demo-bert-tokenizer) introduces the mechanism underlying the algoritm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36437fcd-60e1-4e17-89c3-98a4c24b060c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load BERT tokenizer in\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b0867-06bb-4641-95b7-0eb584192b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(string):\n",
    "    '''Tokenzie the input string with BERT'''\n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    return print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231fc31-0683-41f3-859c-6b6a3618c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abbreviations\n",
    "get_tokens('dlab')\n",
    "\n",
    "# OOV\n",
    "get_tokens('covid')\n",
    "\n",
    "# Prefix\n",
    "get_tokens('huggable')\n",
    "\n",
    "# Digits\n",
    "get_tokens('378')\n",
    "\n",
    "# YOUR EXAMPLE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
