{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794bac7d-1acb-43a7-8e17-1c9fa6e007e1",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Part 1 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c60227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f446a0-61d7-4055-9acb-34b010f8572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Use pandas to import Tweets\n",
    "csv_path = '../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(csv_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8599eb-1e4d-43db-b62c-dce158a18ce0",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o 1: Preprocesamiento con m√∫ltiples pasos\n",
    "\n",
    "Hasta ahora hemos aprendido algunas operaciones de preprocesamiento. ¬°Combin√©moslas en una funci√≥n! Esta funci√≥n te resultar√° √∫til si trabajas con datos de texto en ingl√©s confusos y quieres preprocesarlos con una sola funci√≥n.\n",
    "\n",
    "Se han le√≠do los datos de texto de ejemplo del desaf√≠o 1. Escribe una funci√≥n para:\n",
    "- Convertir el texto en min√∫sculas\n",
    "- Eliminar los signos de puntuaci√≥n\n",
    "- Eliminar los espacios en blanco adicionales\n",
    "\n",
    "¬°Puedes reciclar el c√≥digo que usamos anteriormente!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a63cf0",
   "metadata": {},
   "source": [
    "**Explicaci√≥n del C√≥digo**\n",
    "* Este c√≥digo abre un archivo de texto, lee su contenido completo y lo imprime en pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24bec1d6-ae54-46cb-9db8-0e56b92a30ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This is a text file that has some extra blankspace at the start and end. Blankspace is a catch-all term for spaces, tabs, newlines, and a bunch of other things that computers distinguish but to us all look like spaces, tabs and newlines.\n",
      "\n",
      "\n",
      "The Python method called \"strip\" only catches blankspace at the start and end of a string. But it won't catch it in       the middle,\t\tfor example,\n",
      "\n",
      "in this sentence.\t\tOnce again, regular expressions will\n",
      "\n",
      "help\t\tus    with this.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "challenge1_path = '../data/example1.txt'\n",
    "\n",
    "with open(challenge1_path, 'r') as file:\n",
    "    challenge1 = file.read()\n",
    "    \n",
    "print(challenge1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b610c",
   "metadata": {},
   "source": [
    "**Explicaci√≥n del C√≥digo**\n",
    "- Esta funci√≥n borra todos los signos de puntuaci√≥n del texto, dejando solo letras, n√∫meros y espacios.\n",
    "\n",
    "- **Explicaci√≥n paso a paso**\n",
    "    * from string import punctuation\n",
    "        - Importa una constante llamada punctuation, que contiene todos los caracteres de puntuaci√≥n comunes:\n",
    "        - !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    * Definici√≥n de la funci√≥n remove_punct(text)\n",
    "        - Entrada: una cadena de texto.\n",
    "        - Proceso:\n",
    "            - Recorre cada car√°cter del texto (for char in text).\n",
    "            - Si ese car√°cter no est√° en la lista de punctuation, lo guarda en la lista no_punct.\n",
    "        - Salida: devuelve un nuevo texto (text_no_punct) formado al unir (''.join) todos los caracteres sin puntuaci√≥n.\n",
    "    * Resultado: obtienes el texto original pero sin ning√∫n signo de puntuaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdafd4fc-a816-405f-bfc5-d08285fd4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "    '''Remove punctuation marks in input text'''\n",
    "    \n",
    "    # Select characters not in puncutaion\n",
    "    no_punct = []\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            no_punct.append(char)\n",
    "\n",
    "    # Join the characters into a string\n",
    "    text_no_punct = ''.join(no_punct)   \n",
    "    \n",
    "    return text_no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260273f4",
   "metadata": {},
   "source": [
    "**Explicaci√≥n del C√≥digo**\n",
    "\n",
    "Ese c√≥digo implementa una funci√≥n de limpieza de texto b√°sica en NLP üßπ. Convierte el texto a min√∫sculas, elimina la puntuaci√≥n y normaliza los espacios en blanco.\n",
    "- Explicaci√≥n:\n",
    "    \n",
    "    1 Expresi√≥n regular para espacios: blankspace_pattern = r'\\s+'\n",
    "    - \\s = cualquier espacio en blanco (espacios, tabs, saltos de l√≠nea).\n",
    "    - (+) = uno o m√°s.\n",
    "    + üëâ Este patr√≥n detecta cualquier secuencia de espacios en blanco repetidos.\n",
    "            \n",
    "    2 Texto de reemplazo:\n",
    "    - blankspace_repl = ' '\n",
    "    - Significa que cada secuencia de espacios ser√° reemplazada por un solo espacio.\n",
    "\n",
    "    3 Funci√≥n clean_text:\n",
    "    - Paso 1: text.lower() - Convierte todo a min√∫sculas ‚Üí uniformidad.\n",
    "    - Paso 2: remove_punct(text) - Elimina signos de puntuaci√≥n (usa la funci√≥n que definiste antes).\n",
    "    - Paso 3: \n",
    "        - re.sub(...): reemplaza secuencias de espacios (m√∫ltiples) por uno solo.\n",
    "        - .strip(): quita espacios al inicio y al final.\n",
    "    \n",
    "    4 Devuelve el texto limpio ‚úÖ\n",
    "    5 Ejecuta la limpieza sobre challenge1, que es el texto que le√≠ste desde tu archivo example1.txt.\n",
    "\n",
    "üß™ Ejemplo pr√°ctico\n",
    "- Si challenge1 = \"Hola!!! Mundo, Esto es una PRUEBA...\"\n",
    "    - print(clean_text(challenge1))\n",
    "- Salida:\n",
    "    - hola mundo esto es una prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f8773e7-208c-4272-97e2-061c97860438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a text file that has some extra blankspace at the start and end blankspace is a catchall term for spaces tabs newlines and a bunch of other things that computers distinguish but to us all look like spaces tabs and newlines the python method called strip only catches blankspace at the start and end of a string but it wont catch it in the middle for example in this sentence once again regular expressions will help us with this'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a pattern in regex\n",
    "blankspace_pattern = r'\\s+'\n",
    "\n",
    "# Write a replacement for the pattern identfied\n",
    "blankspace_repl = ' '\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Step 1: Lowercase the input text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 2: Use remove_punct to remove puncutuation marks\n",
    "    text = remove_punct(text)\n",
    "\n",
    "    # Step 3: Remove extra whitespace characters\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "    \n",
    "clean_text(challenge1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24582e",
   "metadata": {},
   "source": [
    "üìå # **Funci√≥n clean_text**\n",
    "\n",
    "* Funci√≥n para:\n",
    "    * Convertir el texto a min√∫sculas\n",
    "    * Eliminar los signos de puntuaci√≥n\n",
    "    * Eliminar los espacios en blanco adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e996ac42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola mundo esto es una prueba\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# patrones para limpiar\n",
    "blankspace_pattern = r'\\s+'\n",
    "blankspace_repl = \" \"\n",
    "\n",
    "def remove_punct(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Step 1: lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: remove punctuation\n",
    "    text = remove_punct(text)\n",
    "    \n",
    "    # Step 3: remove extra whitespace\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Ejemplo de uso\n",
    "challenge1 = \"Hola!!!   Mundo,   esto es   una   PRUEBA...\"\n",
    "print(clean_text(challenge1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f57a143-8876-4305-b3f1-c77001296919",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o 2: Eliminar palabras vac√≠as\n",
    "\n",
    "Ya sabemos c√≥mo funcionan `nltk` y `spaCy` como paquetes de PLN. Tambi√©n hemos demostrado c√≥mo identificar palabras vac√≠as con cada paquete.\n",
    "\n",
    "Escribamos **dos** funciones para eliminar palabras vac√≠as de nuestros datos de texto.\n",
    "\n",
    "- Completar la funci√≥n para eliminar palabras vac√≠as usando `nltk`\n",
    "- El c√≥digo inicial requiere dos argumentos: la entrada de texto sin formato y una lista de palabras vac√≠as predefinidas.\n",
    "- Completar la funci√≥n para eliminar palabras vac√≠as usando `spaCy`\n",
    "- El c√≥digo inicial requiere un argumento: la entrada de texto sin formato.\n",
    "\n",
    "Un recordatorio antes de empezar: ambas funciones toman texto sin formato como entrada; ¬°eso es una se√±al para realizar primero la tokenizaci√≥n del texto sin formato!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f743402-ace4-4b3b-8175-ba8b7ae87197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopword_nltk(raw_text, stopword):\n",
    "    \n",
    "    # Step 1: Tokenization with nltk\n",
    "    tokens = word_tokenize(raw_text)\n",
    "    \n",
    "    # Step 2: Filter out tokens in the stop word list\n",
    "    text = [token for token in tokens if token not in stopword]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c052ec5-2f03-4c45-9f3e-ac0bbf29da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def remove_stopword_spacy(raw_text):\n",
    "\n",
    "    # Step 1: Apply the nlp pipeline\n",
    "    doc = nlp(raw_text)\n",
    "    \n",
    "    # Step 2: Filter out tokens in the stop word list\n",
    "    text = [token.text for token in doc if token.is_stop is False]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93c65-7fb2-40c5-9fc4-bb63fba5c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tweets['text'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1077f4d-e1d5-4d22-a05a-5b8370888aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopword_nltk(text, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec4a692-fa53-4406-a7f7-3ee5e7e9811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopword_spacy(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8c75b",
   "metadata": {},
   "source": [
    "üìå Funci√≥n con NLTK\n",
    "\n",
    "Para ver la salida se debe ejecutar el programa FuncionNLTK.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ced800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "toktok = ToktokTokenizer()\n",
    "\n",
    "# Descargar recursos si no los tienes a√∫n\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords_nltk(text, stop_words):\n",
    "    \"\"\"\n",
    "    Elimina stopwords usando NLTK.\n",
    "    - text: texto en bruto (string)\n",
    "    - stop_words: conjunto/lista de stopwords predefinidas\n",
    "    \"\"\"\n",
    "    # Tokenizaci√≥n\n",
    "    tokens = toktok.tokenize(text.lower())\n",
    "    \n",
    "    # Filtrar palabras que no est√°n en stopwords y son alfab√©ticas\n",
    "        \n",
    "    return [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "\n",
    "# Ejemplo de uso\n",
    "stop_words_es = set(stopwords.words('spanish'))\n",
    "print(remove_stopwords_nltk(\"Hola, este es un ejemplo de texto para probar NLTK.\", stop_words_es))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166671e6",
   "metadata": {},
   "source": [
    "‚úÖ Salida esperada:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56872fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola', 'ejemplo', 'texto', 'probar', 'nltk']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['hola', 'ejemplo', 'texto', 'probar', 'nltk']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd450cd",
   "metadata": {},
   "source": [
    "üìå Funci√≥n con spaCy\n",
    "\n",
    "Para ver la salida se debe ejecutar el programa FuncionSpacy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar modelo en espa√±ol (inst√°lalo con: python -m spacy download es_core_news_sm)\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def remove_stopwords_spacy(text):\n",
    "    \"\"\"\n",
    "    Elimina stopwords usando spaCy.\n",
    "    - text: texto en bruto (string)\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    # Filtrar tokens que no sean stopwords y sean alfab√©ticos\n",
    "    filtered = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Ejemplo de uso\n",
    "print(remove_stopwords_spacy(\"Hola, este es un ejemplo de texto para probar spaCy.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03d606",
   "metadata": {},
   "source": [
    "‚úÖ Salida esperada:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "162e9d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola', 'ejemplo', 'texto', 'probar', 'spacy']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['hola', 'ejemplo', 'texto', 'probar', 'spacy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9a6e3-4658-4556-9bc6-40e58b2045cd",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o 3: Encuentra el l√≠mite de palabras\n",
    "\n",
    "Ahora que sabemos que la tokenizaci√≥n BERT suele devolver subpalabras, ¬°probemos con algunos ejemplos m√°s!\n",
    "\n",
    "¬øTe parece claro el resultado? ¬øCu√°l crees que es el l√≠mite de palabras correcto para dividir las siguientes palabras en subpalabras?\n",
    "\n",
    "Tambi√©n puedes leer m√°s sobre las limitaciones del algoritmo WordPiece. Por ejemplo, [esta entrada de blog](https://medium.com/@rickbattle/weaknesses-of-wordpiece-tokenization-eb20e37fec99) explica en detalle por qu√© falla, y [esta](https://tinkerd.net/blog/machine-learning/bert-tokenization/#demo-bert-tokenizer) presenta el mecanismo subyacente del algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa38a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer in\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b0867-06bb-4641-95b7-0eb584192b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(string):\n",
    "    '''Tokenzie the input string with BERT'''\n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    return print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231fc31-0683-41f3-859c-6b6a3618c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abbreviations\n",
    "get_tokens('dlab')\n",
    "\n",
    "# OOV\n",
    "get_tokens('covid')\n",
    "\n",
    "# Prefix\n",
    "get_tokens('huggable')\n",
    "\n",
    "# Digits\n",
    "get_tokens('378')\n",
    "\n",
    "# YOUR EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7462d696",
   "metadata": {},
   "source": [
    "üìå Funci√≥n bert_tokens.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51bd0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokens.py\n",
    "\n",
    "# Load BERT tokenizer in\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_tokens(string):\n",
    "    '''Tokenize the input string with BERT'''\n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    print(f\"Texto: {string}\")\n",
    "    print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "# Abbreviations\n",
    "get_tokens('dlab')\n",
    "\n",
    "# OOV (Out Of Vocabulary)\n",
    "get_tokens('covid')\n",
    "\n",
    "# Prefix\n",
    "get_tokens('huggable')\n",
    "\n",
    "# Digits\n",
    "get_tokens('378')\n",
    "\n",
    "# Your own example\n",
    "get_tokens('grupo11')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb759cc2",
   "metadata": {},
   "source": [
    "‚úÖ Salida esperada:\n",
    "\n",
    "(.venv312) PS C:\\Users\\fmerino\\Documents\\GitHub\\Python-NLP-Fundamentals> python ./solutions/bert_tokens.py\n",
    "Texto: dlab\n",
    "Tokens: ['dl', '##ab']\n",
    "\n",
    "Texto: covid\n",
    "Tokens: ['co', '##vid']\n",
    "\n",
    "Texto: huggable\n",
    "Tokens: ['hug', '##ga', '##ble']\n",
    "\n",
    "Texto: 378\n",
    "Tokens: ['37', '##8']\n",
    "\n",
    "Texto: grupo11\n",
    "Tokens: ['grupo', '##11']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
