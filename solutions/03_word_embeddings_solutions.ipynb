{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f37149-c4d4-4bb2-be40-33caccccdc92",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Part 3 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428bef9b-2ce9-456c-a260-9b72d79d6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c923bb-7286-4d55-8002-684343a5a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4a748-d43e-4c76-8eaa-ad504e5c3a51",
   "metadata": {},
   "source": [
    "## ðŸ¥Š DesafÃ­o 1: No coinciden\n",
    "\n",
    "Â¡Ahora te toca! En la siguiente celda, hemos preparado una lista de pares de sustantivos con \"cafÃ©\". Por ejemplo, la palabra \"cafÃ©\" se asocia con una bebida de cafÃ© especÃ­fica. AverigÃ¼emos quÃ© bebida de cafÃ© se considera mÃ¡s similar a \"cafÃ©\" y cuÃ¡l no.\n",
    "\n",
    "Completa el bucle \"for\" (dos celdas mÃ¡s abajo) para calcular la similitud de coseno entre cada par de palabras; es decir, usa la funciÃ³n \"similitud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4f076-005a-4176-93dd-60e681f6b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_nouns = [\n",
    "    ('coffee', 'espresso'),\n",
    "    ('coffee', 'cappuccino'),\n",
    "    ('coffee', 'latte'),\n",
    "    ('coffee', 'americano'),\n",
    "    ('coffee', 'irish'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa15ad5-b3fe-4871-8c07-bfa1bac58e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarities between each pair\n",
    "for w1, w2 in coffee_nouns:\n",
    "    similarity = wv.similarity(w1, w2)\n",
    "    print(f\"{w1}, {w2}, {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Si aÃºn no tienes 'wv' cargado, descomenta estas lÃ­neas para usar un modelo pÃºblico:\n",
    "# import gensim.downloader as api\n",
    "# wv = api.load(\"glove-wiki-gigaword-100\")  # o \"word2vec-google-news-300\"\n",
    "\n",
    "coffee_nouns = [\n",
    "    ('coffee', 'espresso'),\n",
    "    ('coffee', 'cappuccino'),\n",
    "    ('coffee', 'latte'),\n",
    "    ('coffee', 'americano'),\n",
    "    ('coffee', 'irish'),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for w1, w2 in coffee_nouns:\n",
    "    # Compatibilidad con Gensim 4.x:\n",
    "    in_vocab = getattr(wv, \"key_to_index\", None)\n",
    "    has_w1 = (w1 in wv.key_to_index) if in_vocab is not None else (w1 in wv)\n",
    "    has_w2 = (w2 in wv.key_to_index) if in_vocab is not None else (w2 in wv)\n",
    "\n",
    "    if not (has_w1 and has_w2):\n",
    "        print(f\"âš ï¸ OOV (fuera de vocabulario): {w1 if not has_w1 else ''} {w2 if not has_w2 else ''}\".strip())\n",
    "        continue\n",
    "\n",
    "    similarity = float(wv.similarity(w1, w2))\n",
    "    results.append((w1, w2, similarity))\n",
    "    print(f\"{w1:>7s} â†” {w2:<11s}: {similarity:.4f}\")\n",
    "\n",
    "# Mostrar el mÃ¡s y el menos similar\n",
    "if results:\n",
    "    most_similar = max(results, key=lambda t: t[2])\n",
    "    least_similar = min(results, key=lambda t: t[2])\n",
    "    print(\"\\nðŸ† MÃ¡s similar a 'coffee':\", most_similar[1], f\"({most_similar[2]:.4f})\")\n",
    "    print(\"ðŸ¥„ Menos similar a 'coffee':\", least_similar[1], f\"({least_similar[2]:.4f})\")\n",
    "else:\n",
    "    print(\"No se pudieron calcular similitudes (todas las palabras OOV).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831cc57",
   "metadata": {},
   "source": [
    "**âœ… Salida esperada:**\n",
    "\n",
    " coffee â†” espresso   : 0.6640  \n",
    " coffee â†” cappuccino : 0.5371  \n",
    " coffee â†” latte      : 0.4755  \n",
    " coffee â†” americano  : 0.0107  \n",
    " coffee â†” irish      : 0.2293  \n",
    "\n",
    "ðŸ† MÃ¡s similar a 'coffee': espresso (0.6640)  \n",
    "ðŸ¥„ Menos similar a 'coffee': americano (0.0107)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df5c2a-3980-43b2-93c0-5b6c1a6b6912",
   "metadata": {},
   "source": [
    "A continuaciÃ³n, investiguemos los verbos comÃºnmente asociados con la preparaciÃ³n de cafÃ©. Analicemos el caso de uso de la funciÃ³n doesnt_match y luego Ãºsela para identificar el verbo que no parece corresponder.\n",
    "\n",
    "Â¡Agregue mÃ¡s verbos a la lista!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8838e-c48f-49f0-9a0c-f135e1fb7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_verbs = ['brew', 'drip', 'pour', 'make', 'grind', 'roast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc36a9d-08c2-47ca-8564-0c3d76fde4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the word that doesn't belong to the list\n",
    "verb_dosent_match = wv.doesnt_match(coffee_verbs)\n",
    "verb_dosent_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc470586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "coffee_verbs = ['brew', 'drip', 'pour', 'make', 'grind', 'roast']\n",
    "\n",
    "# verificar que las palabras existan en el vocabulario\n",
    "print([w for w in coffee_verbs if w not in wv.key_to_index])\n",
    "\n",
    "# encontrar la que no encaja\n",
    "verb_doesnt_match = wv.doesnt_match(coffee_verbs)\n",
    "print(\"Verbo que no encaja:\", verb_doesnt_match)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ab141",
   "metadata": {},
   "source": [
    "**ðŸ”Ž ExplicaciÃ³n paso a paso**\n",
    "\n",
    "El cÃ³digo revisa si los verbos estÃ¡n en el vocabulario del modelo y usa embeddings de palabras para encontrar cuÃ¡l de ellos no pertenece semÃ¡nticamente al grupo. Se usa un modelo de embeddings de palabras (el objeto wv, de Gensim) para detectar quÃ© palabra de la lista no â€œencajaâ€ con las demÃ¡s.\n",
    "\n",
    "Lo que hace cada parte:\n",
    "\n",
    "1. DefiniciÃ³n de la lista  \n",
    "    coffee_verbs = ['brew', 'drip', 'pour', 'make', 'grind', 'roast']  \n",
    "    Se crea una lista de verbos relacionados con la preparaciÃ³n de cafÃ©.\n",
    "\n",
    "2. VerificaciÃ³n de vocabulario  \n",
    "    print([w for w in coffee_verbs if w not in wv.key_to_index])  \n",
    "    wv.key_to_index contiene todas las palabras que conoce el modelo (wv).\n",
    "\n",
    "    Este print muestra cuÃ¡les de los verbos de la lista no existen en el vocabulario del modelo (OOV = out of vocabulary).  \n",
    "    Sirve para saber si habrÃ¡ errores al calcular similitudes.\n",
    "\n",
    "3. Encontrar el â€œintrusoâ€  \n",
    "    verb_doesnt_match = wv.doesnt_match(coffee_verbs)  \n",
    "    doesnt_match compara todos los embeddings de la lista.\n",
    "\n",
    "    Calcula quÃ© palabra tiene la menor similitud promedio con las demÃ¡s.  \n",
    "    Esa palabra se considera la que â€œno perteneceâ€ al grupo.\n",
    "\n",
    "4. Mostrar resultado  \n",
    "    print(\"Verbo que no encaja:\", verb_doesnt_match)\n",
    "\n",
    "    Imprime el verbo detectado como intruso.\n",
    "\n",
    "    En este caso, lo mÃ¡s comÃºn es que devuelva \"make\", porque es un verbo muy genÃ©rico, mientras que los demÃ¡s estÃ¡n mÃ¡s ligados a preparar cafÃ©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc77160",
   "metadata": {},
   "source": [
    "**âœ… Salida esperada:**\n",
    "\n",
    "**Verbo que no encaja: make**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ac569-8889-4dbd-aa5d-b0ae7314a0aa",
   "metadata": {},
   "source": [
    "## ðŸ¥Š DesafÃ­o 2: Â¿Mujer es ama de casa?\n",
    "\n",
    "[Bolukbasi et al. (2016)](https://arxiv.org/pdf/1607.06520) es una investigaciÃ³n exhaustiva sobre el sesgo de gÃ©nero presente en las incrustaciones de palabras, y se centra principalmente en las analogÃ­as de palabras, especialmente aquellas que revelan estereotipos de gÃ©nero. Analicemos un par de ejemplos analizados en el artÃ­culo, utilizando la funciÃ³n `most_similiar` que acabamos de aprender.\n",
    "\n",
    "El siguiente bloque de cÃ³digo contiene algunos ejemplos que podemos pasar al argumento `positive`: queremos que la salida sea similar a, por ejemplo, `woman` y `chairman`, y mientras tanto, tambiÃ©n especificamos que debe ser diferente a `man`. Imprimiremos el resultado superior indexando al elemento 0.\n",
    "\n",
    "Completemos el siguiente bucle `for`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4082d7e-9b2e-43dc-b7fa-b847889760be",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pair = [['woman', 'chairman'],\n",
    "                 ['woman', 'doctor'], \n",
    "                 ['woman', 'computer_programmer']]\n",
    "negative_word = 'man'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9a002-fd48-4f8f-b677-842c8538a560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most similar word given positive and negative examples\n",
    "for example in positive_pair:\n",
    "    result = wv.most_similar(positive=example, negative=negative_word)\n",
    "    print(f\"man is to {example[1]} as woman is to {result[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ed50f",
   "metadata": {},
   "source": [
    "**ðŸ“Œ FunciÃ³n ama_de_casa.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5618031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# pares positivos (woman + target)\n",
    "positive_pair = [\n",
    "    ['woman', 'chairman'],\n",
    "    ['woman', 'doctor'],\n",
    "    ['woman', 'computer_programmer'],\n",
    "]\n",
    "negative_word = 'man'\n",
    "\n",
    "# --- ComprobaciÃ³n de vocabulario ---\n",
    "needed = {negative_word}\n",
    "for p in positive_pair:\n",
    "    needed.update(p)\n",
    "\n",
    "oov = [w for w in needed if w not in wv.key_to_index]\n",
    "if oov:\n",
    "    print(\"âš ï¸ Palabras fuera de vocabulario:\", oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0cd95",
   "metadata": {},
   "source": [
    "**ðŸ”Ž ExplicaciÃ³n paso a paso**\n",
    "\n",
    "Este cÃ³digo prepara un experimento de analogÃ­as con word embeddings y verifica que las palabras necesarias existan en el vocabulario del modelo antes de ejecutarlo.\n",
    "\n",
    "Resumen paso a paso:\n",
    "\n",
    "1. Importa librerÃ­as y carga un modelo preentrenado  \n",
    "    wv = KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "- Abre el modelo GoogleNews (â‰ˆ1.5 GB) en formato Word2Vec binario.  \n",
    "- Si la ruta/archivo no existe, lanzarÃ¡ FileNotFoundError.  \n",
    "- Requiere bastante RAM.  \n",
    "2. Define pares â€œpositivosâ€ para analogÃ­as del tipo:\n",
    "    â€œwoman + chairman âˆ’ man â‰ˆ ?â€  \n",
    "    â€œwoman + doctor âˆ’ man â‰ˆ ?â€  \n",
    "    â€œwoman + computer_programmer âˆ’ man â‰ˆ ?â€\n",
    "\n",
    "3. Palabra â€œnegativaâ€:  \n",
    "    negative_word = 'man'\n",
    "\n",
    "4. Verifica vocabulario (OOV):  \n",
    "    Crea el conjunto de todas las palabras que se usarÃ¡n y lista las que no estÃ¡n en el vocabulario del modelo (wv.key_to_index):  \n",
    "        oov = [w for w in needed if w not in wv.key_to_index]  \n",
    "        if oov:  \n",
    "            print(\"âš ï¸ Palabras fuera de vocabulario:\", oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df5a4f",
   "metadata": {},
   "source": [
    "**âœ… Salida esperada:**  \n",
    "man is to chairman as woman is to chairwoman (score=0.7713)  \n",
    "man is to doctor as woman is to gynecologist (score=0.7094)  \n",
    "man is to computer_programmer as woman is to homemaker (score=0.5627)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0142b3-545b-4f52-8e7b-2711e20abbd6",
   "metadata": {},
   "source": [
    "## ðŸ¥Š DesafÃ­o 3: Construir un Eje SemÃ¡ntico\n",
    "\n",
    "Â¡Ahora te toca! Tenemos dos conjuntos de palabras clave para \"female\" y \"male\". Estos son ejemplos de palabras probadas en Bolukbasi et al., 2016. Obtendremos las incrustaciones de estas palabras de glove para calcular el eje de gÃ©nero.\n",
    "\n",
    "La celda de la funciÃ³n `get_semaxis` proporciona cÃ³digo inicial. Completa la funciÃ³n. Si todo se ejecuta, el tamaÃ±o de la incrustaciÃ³n del eje semÃ¡ntico deberÃ­a ser igual al tamaÃ±o del vector de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72fb395-796d-410a-a3fa-f14c55b36f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac1297-5aca-4cdf-bd2b-bd0c71cfd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two sets of pole words (examples from Bolukbasi et al., 2016)\n",
    "female = ['she', 'woman', 'female', 'daughter', 'mother', 'girl']\n",
    "male = ['he', 'man', 'male', 'son', 'father', 'boy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d6225-015a-4ad9-98e6-f05a4aabbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semaxis(list1, list2, model, embedding_size):\n",
    "    '''Calculate the embedding of a semantic axis given two lists of pole words.'''\n",
    "\n",
    "    # STEP 1: Get the embeddings for terms in each list\n",
    "    v_plus = [model[term] for term in list1]\n",
    "    v_minus = [model[term] for term in list2]\n",
    "\n",
    "    # Step 2: Calculate the mean embeddings for each list\n",
    "    v_plus_mean = np.mean(v_plus, axis=0)\n",
    "    v_minus_mean = np.mean(v_minus, axis=0)\n",
    "\n",
    "    # Step 3: Get the difference between two means\n",
    "    sem_axis = v_plus_mean - v_minus_mean\n",
    "\n",
    "    # Sanity check\n",
    "    assert sem_axis.size == embedding_size\n",
    "    \n",
    "    return sem_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe27243-1c45-4433-8f33-559c121c98b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08418201,  0.30625182, -0.23662159,  0.02026337, -0.00296998,\n",
       "        0.6195349 ,  0.01208681,  0.06963003,  0.49099812, -0.20878893,\n",
       "        0.00934163, -0.44707334,  0.48806185,  0.19471335,  0.20141667,\n",
       "        0.0832995 , -0.4245833 , -0.08612835,  0.47612852, -0.05129966,\n",
       "        0.31475997,  0.49075842,  0.12465019,  0.26685053,  0.29776838,\n",
       "        0.14211655, -0.09953564,  0.2320785 , -0.01026282, -0.30585438,\n",
       "       -0.1335001 ,  0.21605133,  0.10961549, -0.03373036, -0.13584831,\n",
       "       -0.12131716, -0.14671612, -0.04348468,  0.06151834, -0.3654362 ,\n",
       "       -0.06193466, -0.17093089,  0.5058871 , -0.44872418,  0.05962732,\n",
       "       -0.18274659,  0.24432765, -0.3396697 ,  0.00442566,  0.10554916],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plug in the gender lists to calculate the semantic axis for gender\n",
    "gender_axis = get_semaxis(list1=female, \n",
    "                          list2=male, \n",
    "                          model=glove, \n",
    "                          embedding_size=50)\n",
    "gender_axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ed6323",
   "metadata": {},
   "source": [
    "\n",
    "Â¡Listo! AquÃ­ tienes la funciÃ³n get_semaxis completa y un poco mÃ¡s robusta (filtra OOV y opcionalmente normaliza el eje). Con GloVe-50, el vector resultante queda de tamaÃ±o 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cdd8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Cargar GloVe 50D\n",
    "glove = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "# Polos (Bolukbasi et al., 2016)\n",
    "female = ['she', 'woman', 'female', 'daughter', 'mother', 'girl']\n",
    "male   = ['he', 'man',   'male',   'son',      'father', 'boy']\n",
    "\n",
    "def get_semaxis(list1, list2, model, embedding_size, normalize=True):\n",
    "    \"\"\"Calcula el embedding de un eje semÃ¡ntico dadas dos listas de palabras polo.\"\"\"\n",
    "    # 1) Filtrar palabras fuera de vocabulario (OOV)\n",
    "    in_vocab1 = [w for w in list1 if w in model.key_to_index]\n",
    "    in_vocab2 = [w for w in list2 if w in model.key_to_index]\n",
    "    if not in_vocab1 or not in_vocab2:\n",
    "        raise ValueError(\"Alguna de las listas quedÃ³ vacÃ­a tras filtrar palabras OOV.\")\n",
    "\n",
    "    # 2) Obtener embeddings y promediar cada polo\n",
    "    v_plus  = np.vstack([model[w] for w in in_vocab1])   # polo positivo\n",
    "    v_minus = np.vstack([model[w] for w in in_vocab2])   # polo negativo\n",
    "    v_plus_mean  = v_plus.mean(axis=0)\n",
    "    v_minus_mean = v_minus.mean(axis=0)\n",
    "\n",
    "    # 3) Diferencia de medios = eje semÃ¡ntico\n",
    "    sem_axis = v_plus_mean - v_minus_mean\n",
    "\n",
    "    # (Opcional) normalizar a unidad, Ãºtil para proyecciones consistentes\n",
    "    if normalize:\n",
    "        norm = np.linalg.norm(sem_axis)\n",
    "        if norm > 0:\n",
    "            sem_axis = sem_axis / norm\n",
    "\n",
    "    # Sanity check\n",
    "    assert sem_axis.size == embedding_size\n",
    "    return sem_axis\n",
    "\n",
    "# Calcular el eje de gÃ©nero\n",
    "gender_axis = get_semaxis(\n",
    "    list1=female,\n",
    "    list2=male,\n",
    "    model=glove,\n",
    "    embedding_size=50\n",
    ")\n",
    "\n",
    "gender_axis, gender_axis.shape  # -> (array([...]), (50,))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
